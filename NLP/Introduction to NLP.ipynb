{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One common task in NLP (Natural Language Processing) is tokenization. \"Tokens\" are usually individual words (at least in languages like English) and \"tokenization\" is taking a text or set of text and breaking it up into its individual words. These tokens are then used as the input for other types of analysis or tasks, like parsing (automatically tagging the syntactic relationship between words)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = 'NLP.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1a. New libraries used in this section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'super', 'awesome', '!']\n"
     ]
    }
   ],
   "source": [
    "example_line = 'This is super awesome!'\n",
    "print(word_tokenize(example_line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This is super awesome!', 'Even though I have no idea what I am doing!', 'But still!']\n"
     ]
    }
   ],
   "source": [
    "example_paragraph = 'This is super awesome! Even though I have no idea what I am doing! But still!'\n",
    "print(sent_tokenize(example_paragraph))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### While stopwords removes noise from the sentences, note that it might totally change the meaning of the sentence. Take for example the word 'not'. This is found as a stopword in the default stopword corpora. Therefore, you might want to edit it out. If you want to find the directory, you might have to use nltk.download() to find the download directory associated with the NLTK package. From there, you want to find nltk_data -> corpora -> stopwords -> english and open using a text editor. This will then allow you to add or remove stop words. ('C:\\Users\\esnxwng\\AppData\\Roaming\\nltk_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2a. New libraries used in this section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\n",
      "not\n",
      "big\n",
      "fan\n",
      "Twitter\n",
      "Analysis\n",
      "!\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "example_line = 'I am not a big fan of Twitter Analysis!'\n",
    "\n",
    "words = word_tokenize(example_line)\n",
    "\n",
    "for word in words:\n",
    "    \n",
    "    if word not in stop_words:\n",
    "        \n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Part of Speech Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In corpus linguistics, part-of-speech tagging (POS tagging or PoS tagging or POST), also called grammatical tagging or word-category disambiguation, is the process of marking up a word in a text (corpus) as corresponding to a particular part of speech,[1] based on both its definition and its context—i.e., its relationship with adjacent and related words in a phrase, sentence, or paragraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_paragraph = 'Thank you very much. Mr. Speaker, Mr. President, distinguished members of Congress, honored guests and fellow citizens. May I congratulate all of you who are members of this historic 100th Congress of the United States of America. In this 200th anniversary year of our Constitution, you and I stand on the shoulders of giants–men whose words and deeds put wind in the sails of freedom.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('Thank', 'NNP'), ('you', 'PRP'), ('very', 'RB'), ('much', 'RB'), ('.', '.'), ('Mr.', 'NNP'), ('Speaker', 'NNP'), (',', ','), ('Mr.', 'NNP'), ('President', 'NNP'), (',', ','), ('distinguished', 'VBD'), ('members', 'NNS'), ('of', 'IN'), ('Congress', 'NNP'), (',', ','), ('honored', 'VBD'), ('guests', 'NNS'), ('and', 'CC'), ('fellow', 'JJ'), ('citizens', 'NNS'), ('.', '.'), ('May', 'NNP'), ('I', 'PRP'), ('congratulate', 'VBP'), ('all', 'DT'), ('of', 'IN'), ('you', 'PRP'), ('who', 'WP'), ('are', 'VBP'), ('members', 'NNS'), ('of', 'IN'), ('this', 'DT'), ('historic', 'JJ'), ('100th', 'JJ'), ('Congress', 'NNP'), ('of', 'IN'), ('the', 'DT'), ('United', 'NNP'), ('States', 'NNPS'), ('of', 'IN'), ('America', 'NNP'), ('.', '.'), ('In', 'IN'), ('this', 'DT'), ('200th', 'CD'), ('anniversary', 'JJ'), ('year', 'NN'), ('of', 'IN'), ('our', 'PRP$'), ('Constitution', 'NNP'), (',', ','), ('you', 'PRP'), ('and', 'CC'), ('I', 'PRP'), ('stand', 'VBP'), ('on', 'IN'), ('the', 'DT'), ('shoulders', 'NNS'), ('of', 'IN'), ('giants–men', 'NNS'), ('whose', 'WP$'), ('words', 'NNS'), ('and', 'CC'), ('deeds', 'NNS'), ('put', 'VBD'), ('wind', 'NN'), ('in', 'IN'), ('the', 'DT'), ('sails', 'NNS'), ('of', 'IN'), ('freedom', 'NN'), ('.', '.')]]\n"
     ]
    }
   ],
   "source": [
    "tagged = []\n",
    "words = []\n",
    "\n",
    "#In the first step, sent_tokenize is used to break a paragraph down into lines\n",
    "for lines in sent_tokenize(example_paragraph):\n",
    "    \n",
    "    #In the second step, word_tokenize is used to break lines down into words\n",
    "    for word in word_tokenize(lines):\n",
    "        \n",
    "        #The broken down words of the paragraph is then appended to the list called words\n",
    "        words.append(word)\n",
    "\n",
    "#part of speech tagging is then used to tag entities\n",
    "tagged.append(nltk.pos_tag(words))\n",
    "print(tagged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Stemming & Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For grammatical reasons, documents are going to use different forms of a word, such as organize, organizes, and organizing. Additionally, there are families of derivationally related words with similar meanings, such as democracy, democratic, and democratization. In many situations, it seems as if it would be useful for a search for one of these words to return documents that contain another word in the set.\n",
    "\n",
    "#### Similarities between stemming and lemmatization:- \n",
    "\n",
    "##### The goal of both stemming and lemmatization is to reduce inflectional forms and sometimes derivationally related forms of a word to a common base form.\n",
    "\n",
    "#### Differences between stemming and lemmatization:- \n",
    "\n",
    "##### A. Stemming usually refers to a crude heuristic process that chops off the ends of words in the hope of achieving this goal correctly most of the time, and often includes the removal of derivational affixes. \n",
    "\n",
    "##### B. Lemmatization usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma .\n",
    "\n",
    "##### If confronted with the token saw, stemming might return just s, whereas lemmatization would attempt to return either see or saw depending on whether the use of the token was as a verb or a noun. The two may also differ in that stemming most commonly collapses derivationally related words, whereas lemmatization commonly only collapses the different inflectional forms of a lemma."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4a. New libraries used in this section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4b. Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "\n",
    "stem_words = []\n",
    "\n",
    "for word in words:\n",
    "    \n",
    "    stem_words.append(ps.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['thank', 'you', 'veri', 'much', '.', 'mr.', 'speaker', ',', 'mr.', 'presid', ',', 'distinguish', 'member', 'of', 'congress', ',', 'honor', 'guest', 'and', 'fellow', 'citizen', '.', 'may', 'I', 'congratul', 'all', 'of', 'you', 'who', 'are', 'member', 'of', 'thi', 'histor', '100th', 'congress', 'of', 'the', 'unit', 'state', 'of', 'america', '.', 'In', 'thi', '200th', 'anniversari', 'year', 'of', 'our', 'constitut', ',', 'you', 'and', 'I', 'stand', 'on', 'the', 'shoulder', 'of', 'giants–men', 'whose', 'word', 'and', 'deed', 'put', 'wind', 'in', 'the', 'sail', 'of', 'freedom', '.']\n"
     ]
    }
   ],
   "source": [
    "print(stem_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4c. Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Thank', 'you', 'very', 'much', '.', 'Mr.', 'Speaker', ',', 'Mr.', 'President', ',', 'distinguished', 'member', 'of', 'Congress', ',', 'honored', 'guest', 'and', 'fellow', 'citizen', '.', 'May', 'I', 'congratulate', 'all', 'of', 'you', 'who', 'are', 'member', 'of', 'this', 'historic', '100th', 'Congress', 'of', 'the', 'United', 'States', 'of', 'America', '.', 'In', 'this', '200th', 'anniversary', 'year', 'of', 'our', 'Constitution', ',', 'you', 'and', 'I', 'stand', 'on', 'the', 'shoulder', 'of', 'giants–men', 'whose', 'word', 'and', 'deed', 'put', 'wind', 'in', 'the', 'sail', 'of', 'freedom', '.']\n"
     ]
    }
   ],
   "source": [
    "lm = WordNetLemmatizer()\n",
    "\n",
    "lemma_words = []\n",
    "\n",
    "for word in words:\n",
    "    \n",
    "    lemma_words.append(lm.lemmatize(word))\n",
    "    \n",
    "print(lemma_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Frequency Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sometimes we want to know the occurrence of a word in an article or most common 15 words in an article, then we use the FreqDist() function from NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_dist = (nltk.FreqDist(stem_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('of', 8), ('.', 4), (',', 4), ('you', 3), ('and', 3)]\n"
     ]
    }
   ],
   "source": [
    "print(words_dist.most_common(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. WordNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wordnet is a huge collection of synsets, meanings, definition, examples, synonyms, antonyms etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6a. New libraries used in this section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = wordnet.synsets('big')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "above average in size or number or quantity or magnitude or extent\n"
     ]
    }
   ],
   "source": [
    "print(words[0].definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a large city', 'set out for the big city', 'a large sum', 'a big (or large) barn', 'a large family', 'big businesses', 'a big expenditure', 'a large number of newspapers', 'a big group of scientists', 'large areas of the world']\n"
     ]
    }
   ],
   "source": [
    "print(words[0].examples())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A synonym is a word or phrase that means exactly or nearly the same as another lexeme in the same language. Words that are synonyms are said to be synonymous, and the state of being a synonym is called synonymy.\n",
    "\n",
    "#### An antonyms is a word of opposite meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synonyms: {'adult', 'big', 'magnanimous', 'crowing', 'bounteous', 'grownup', 'prominent', 'braggy', 'bragging', 'self-aggrandizing', 'vauntingly', 'boastfully', 'fully_grown', 'liberal', 'cock-a-hoop', 'great', 'vainglorious', 'heavy', 'grown', 'swelled', 'handsome', 'with_child', 'openhanded', 'bighearted', 'enceinte', 'self-aggrandising', 'bountiful', 'boastful', 'bad', 'expectant', 'large', 'braggart', 'freehanded', 'gravid', 'full-grown', 'giving'}\n",
      "\n",
      "Antonyms: {'little', 'small'}\n"
     ]
    }
   ],
   "source": [
    "synonyms = []\n",
    "antonyms = []\n",
    "\n",
    "for words in wordnet.synsets('big'):\n",
    "    \n",
    "    for word in words.lemmas():\n",
    "        \n",
    "        synonyms.append(word.name())\n",
    "        \n",
    "        if word.antonyms():\n",
    "            \n",
    "            antonyms.append(word.antonyms()[0].name())\n",
    "            \n",
    "print('Synonyms: {}'.format(set(synonyms)))\n",
    "print()\n",
    "print('Antonyms: {}'.format(set(antonyms)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Therefore, the synsets that we were investigating, was with regards to the word 'BIG'. This would result in the antonyms to be 'SMALL'."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
