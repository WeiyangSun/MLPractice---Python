{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Importing in Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Setting YouTube URL Link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To find the needed URL, we have to first access YouTube and search. In this case, we searched 'Travel'. This will redirect us to a page full of results. On the top, there is a filter button, which is used to select only 'Videos'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The webdriver.Chrome() command launches open the web browser selected and awaits further information. In this case, our URL link."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The URL link is provided to the driver using driver.get('insert_url_link_here'). This will bring the blank web browser to the specified url. The youtube link follows a fixed format -> search_query='topic of interest' followed by sp = 'EgIQAQ%253D%253D'. SP refers to the filter type. In this case EgIQAQ%253D%253D means only videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get('https://www.youtube.com/results?search_query=travel&sp=EgIQAQ%253D%253D')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In order to fetch all the video links present and create a list to store these links, we have to first inspect the elements in the browser window. This can be achieved through the F12 key. Search for the anchor tag with id = ”video-title” and then right-click on it -> Copy -> XPath. The XPath should look something like : //*[@id=”video-title”]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_data = driver.find_elements_by_xpath('//*[@id=\"video-title\"]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With the previous step, we have segmented out all the HTML portions with the video information out from the original website. Now, we need to run through the remaining HTML data and obtain the URL using the get_attribute('href') function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "individual_video_links = []\n",
    "\n",
    "for each_video in video_data:\n",
    "    \n",
    "    individual_video_links.append(each_video.get_attribute('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "individual_video_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "individual_video_titles = []\n",
    "\n",
    "for each_video in video_data:\n",
    "    \n",
    "    individual_video_titles.append(each_video.get_attribute('title'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "individual_video_titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2b. To get links for multiple topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List container to hold all the subsequent links obtained\n",
    "individual_video_links = []\n",
    "individual_video_topics = []\n",
    "\n",
    "#Interested topic list\n",
    "topic_list = ['game coc', 'travel singapore', 'food singapore']\n",
    "\n",
    "#To replicate Youtube search engine, which takes spaces in search terms and replace it with a '+'\n",
    "for each_topic in topic_list:\n",
    "    \n",
    "    #If a search term is more than 2 words long, then it will have to be split, and a '+' have to be added to join the words together\n",
    "    if len(each_topic.split(' ')) > 1:\n",
    "        \n",
    "        length_of_topic = len(each_topic.split(' ')) #this variable helps us by creating an index for the join later\n",
    "        individual_words_in_topic = each_topic.split(' ') #this variable creates the split words in the search term\n",
    "        \n",
    "        #List container to hold the broken down words of a search term\n",
    "        pieced_up_topic = []\n",
    "        \n",
    "        for i in range(0, length_of_topic):\n",
    "        \n",
    "            pieced_up_topic.append(individual_words_in_topic[i])\n",
    "        \n",
    "        #Finally, piecing it all up together with '+' in between instead of spaces\n",
    "        final_edited_topic = '+'.join(pieced_up_topic)\n",
    "        \n",
    "        #Chromedriver will now automatically key in the search URL into the browser and retrieve the appropriate website\n",
    "        driver.get('https://www.youtube.com/results?search_query={}&sp=EgIQAQ%253D%253D'.format(final_edited_topic))\n",
    "        \n",
    "        #Only the video information from YouTube search page is extracted out\n",
    "        topic_video_data = driver.find_elements_by_xpath('//*[@id=\"video-title\"]')\n",
    "        \n",
    "        #However, each search page contains multiple video links, therefore we need to iterate through the list and open each individually\n",
    "        for each_video in topic_video_data:\n",
    "\n",
    "            individual_video_links.append(each_video.get_attribute('href'))\n",
    "            individual_video_topics.append(each_topic)\n",
    "    \n",
    "    #This section is only used when the search term contain only 1 word\n",
    "    else:\n",
    "        \n",
    "        driver.get('https://www.youtube.com/results?search_query={}&sp=EgIQAQ%253D%253D'.format(each_topic))\n",
    "        topic_video_data = driver.find_elements_by_xpath('//*[@id=\"video-title\"]')\n",
    "        \n",
    "        for each_video in topic_video_data:\n",
    "            \n",
    "            individual_video_links.append(each_video.get_attribute('href'))\n",
    "            individual_video_topics.append(each_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_merged_list = pd.DataFrame({'first_column':individual_video_links, 'second_column':individual_video_topics})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_merged_list = pre_merged_list.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### There are scenarios where a 'None' result is obtained. This could probably be from video advertisers found on the YouTube search page itself. Nevertheless, this might be a trivial scenario and a simple list comprehension should filter out the 'None' values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled_video_links = [each_video_link for each_video_link in individual_video_links if each_video_link]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The secondary problem with parsing in YouTube video data is that YouTube search results page adopts a infinite scroll and loading mechanism until Amazon webpages which requires you to click on the next page button. Therefore, the Selenium code for such a mechanism is as follows:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "SCROLL_PAUSE_TIME = 0.5\n",
    "\n",
    "# Get scroll height\n",
    "last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "while True:\n",
    "    # Scroll down to bottom\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "    # Wait to load page\n",
    "    time.sleep(SCROLL_PAUSE_TIME)\n",
    "\n",
    "    # Calculate new scroll height and compare with last scroll height\n",
    "    new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    if new_height == last_height:\n",
    "        break\n",
    "    last_height = new_height"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Creating DataFrame to store Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns = ['link', 'title', 'description', 'category'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Scraping Video Details from Youtube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# “wait” will ignore instances of NotFoundException that are encountered (thrown) by default in the ‘until’ condition. It will immediately propagate all others.\n",
    "wait = WebDriverWait(driver, 15) # 10 means timeout in 10 seconds when an exception is called\n",
    "\n",
    "i = 0\n",
    "\n",
    "for each_link in compiled_video_links:\n",
    "    \n",
    "    driver.get(each_link)\n",
    "    \n",
    "    video_id = each_link.strip('https://www.youtube.com/watch?v=')\n",
    "    video_title = wait.until(EC.presence_of_element_located(\n",
    "                    (By.CSS_SELECTOR, 'h1.title yt-formatted-string'))).text\n",
    "    video_description = wait.until(EC.presence_of_element_located(\n",
    "                    (By.CSS_SELECTOR, 'div#description yt-formatted-string'))).text\n",
    "    \n",
    "    df.loc[len(df)] = [video_id, video_title, video_description, pre_merged_list.iloc[i,1]]\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Preparing Data for clean up using NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### It is recommended to store each column seperately so that we can perform easier cleaning up quickly and easily. We begin this step by first creating multiple empty dataframes with the appropriate column names. At this stage, the row values are all null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_link = pd.DataFrame(columns = [\"link\"])        \n",
    "df_title = pd.DataFrame(columns = [\"title\"])        \n",
    "df_description = pd.DataFrame(columns = [\"description\"])        \n",
    "df_category = pd.DataFrame(columns = [\"category\"])        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Since all the row values are null, we now need to reference to our original dataframe to obtain the values to replace the multiple empty dataframe!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_link['link'] = df['link'] \n",
    "df_title ['title']= df['title'] \n",
    "df_description['description'] = df['description'] \n",
    "df_category['category'] = df['category']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing in the necessary NLTK libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re #regular expression\n",
    "import nltk #NLP library\n",
    "nltk.download('stopwords') #downloading dictionary of stopwords which are common words in a language that add no intrinsic value to the sentence meaning\n",
    "from nltk.corpus import stopwords #loading the stopwords into python\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Cleaning up Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PorterStemmer is the function to stemming. The idea of stemming is a sort of normalizing method. Many variations of words carry the same meaning, other than when tense is involved. The reason why we stem is to shorten the lookup, and normalize sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To clean up the title segment of the data scrapped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_corpus = []\n",
    "\n",
    "for i in range(0, len(df)):\n",
    "    \n",
    "    title = re.sub('[^a-zA-Z]', ' ', df_title['title'][i])\n",
    "    title = title.lower()\n",
    "    title = title.split()\n",
    "    \n",
    "    title = [ps.stem(word) for word in title if not word in set(stopwords.words('english'))]\n",
    "    title = ' '.join(title)\n",
    "    title_corpus.append(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To clean up the description segment of the data scrapped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "description_corpus = []\n",
    "\n",
    "for i in range(0, len(df)): #this creates the indexing of each row of description\n",
    "    \n",
    "    review = re.sub('[^a-zA-Z]', ' ', df_description['description'][i]) #regular expression operation\n",
    "    review = review.lower() #changing all letters to lowercase\n",
    "    review = review.split() #splitting each word\n",
    "        \n",
    "    review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))] #if words are not part of the stopword dictionary, then it will be stemmed\n",
    "    review = ' '.join(review) #and joined with a space\n",
    "    description_corpus.append(review) #and finally appended to a list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting it back into a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftitle = pd.DataFrame({'title': title_corpus})\n",
    "dfdescription = pd.DataFrame({'description': description_corpus})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Label Encoding Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfcategory = df_category.apply(LabelEncoder().fit_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Final Dataset for ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = pd.concat([df_link, dftitle, dfdescription, dfcategory], axis = 1, join_axes = [df_link.index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a bag-of-words for our model to understand the keywords to classify videos accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(max_features = 1500) #The number of features created from data stored in the corpus list\n",
    "\n",
    "X = cv.fit_transform(title_corpus, description_corpus).toarray() #This stores all the features\n",
    "y = df_final.iloc[:, 3].values #This stores the encoded categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now that we have our X's values and Y's values, we can begin building the model for ML. We begin first splitting the X and y into training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = RandomForestClassifier(n_estimators = 1000, criterion = 'entropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import *\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
